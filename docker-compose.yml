# Docker Compose Stack for Local Voice MCP Agent
# Target: Dual RTX 3090/4090 GPUs (24GB VRAM each)
#
# Services:
#   - vllm: LLM inference server (Devstral 24B, tensor-parallel across both GPUs)
#   - voice-agent: FastAPI server with Whisper ASR (cuda:0) + VibeVoice TTS (cuda:1)
#
# Usage:
#   docker compose build
#   docker compose up -d
#   open http://localhost:3000

services:
  # ===========================================================================
  # vLLM - LLM Inference Server
  # Uses tensor-parallel-size=2 across both GPUs
  # Memory: ~70% of each GPU (~16.8GB per GPU)
  # ===========================================================================
  vllm:
    image: vllm/vllm-openai:nightly
    container_name: jarvis-vllm
    command:
      - --model
      - mistralai/Devstral-Small-2-24B-Instruct-2512
      - --max_model_len
      - "49000"
      - --gpu-memory-utilization
      - "0.75"
      - --served-model-name
      - devstral-2-24b
      - --tool-call-parser
      - mistral
      - --enable-auto-tool-choice
      - --tensor-parallel-size
      - "2"
      - --host
      - "0.0.0.0"
      - --port
      - "8000"
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface:rw
    environment:
      - CUDA_VISIBLE_DEVICES=0,1
      - HF_HOME=/root/.cache/huggingface
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
    networks:
      - jarvis-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
    ipc: host

  # ===========================================================================
  # Voice Agent - Main Application
  # FastAPI + Whisper ASR (cuda:0) + VibeVoice TTS (cuda:1)
  # Memory: ASR ~6GB on GPU 0, TTS ~2-3GB on GPU 1 (shares headroom with vLLM)
  # ===========================================================================
  voice-agent:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: jarvis-voice-agent
    ports:
      - "3000:3000"
    volumes:
      # HuggingFace model cache (shared with vLLM for efficiency)
      - ~/.cache/huggingface:/root/.cache/huggingface:rw
    environment:
      # GPU visibility
      - CUDA_VISIBLE_DEVICES=0,1
      - NVIDIA_VISIBLE_DEVICES=all
    networks:
      - jarvis-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
    ipc: host

networks:
  jarvis-net:
    driver: bridge
    name: jarvis-network
